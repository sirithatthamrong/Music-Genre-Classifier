{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classifier by Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/chadakeng/Desktop/NumCom/project/Music-Genre-Classifier/features_30_sec.csv')\n",
    "# print(df.head)\n",
    "\n",
    "# Filter for the genres of interest\n",
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "# filters the DataFrame df to include only the rows where the values in the filename (head) match any of the values in the list genres that way we can train\n",
    "# each genre seperately\n",
    "df = df[df['label'].isin(genres)]\n",
    "\n",
    "# Split the dataset into features (X) and target (y), then into training and test sets\n",
    "X = df.drop(['filename', 'label'], axis=1)  # Drop non-numeric columns\n",
    "y = df['label']\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1000) # we keep some of the data for training and the rest goes to testing.\n",
    "\n",
    "# print(X_train.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "Note: will need multinomial logistic regression for this https://www.youtube.com/watch?v=Mi992wr6zKc&ab_channel=GopalPrasadMalakar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft max function generalizes sigmoid function for multiple classes (jazz rock pop blues etc instead of being just 1 or 0 for one genre)\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegression():\n",
    "    # constructor\n",
    "    def __init__(self, lr=0.001, n_iters=1000):\n",
    "        self.lr = lr # learning rate\n",
    "        self.n_iters = n_iters # number of iterations for the gradient descent loop\n",
    "        self.weights = None # we dont know how much each factor weighs in to determining what genre it is so we initialize as none\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape # X.shape returns a tuple representing the dimensions of X\n",
    "        self.classes_ = np.unique(y) \n",
    "        n_classes = len(self.classes_) # how many different genres we have\n",
    "        \n",
    "        self.weights = np.zeros((n_classes, n_features))\n",
    "        self.bias = np.zeros(n_classes)\n",
    "        \n",
    "        y_encoded = self._one_hot(y) # converts the categorical genre labels into a binary matrix. e.g.(rock, pop, jazz)->[1, 0, 0]\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            linear_pred = np.dot(X, self.weights.T) + self.bias\n",
    "            y_pred = softmax(linear_pred)\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y_encoded))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y_encoded, axis=0)\n",
    "            \n",
    "            self.weights -= self.lr * dw.T\n",
    "            self.bias -= self.lr * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_pred = np.dot(X, self.weights.T) + self.bias\n",
    "        y_pred = softmax(linear_pred)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    def _one_hot(self, y):\n",
    "        y_encoded = np.zeros((len(y), len(self.classes_)))\n",
    "        for idx, label in enumerate(self.classes_):\n",
    "            y_encoded[np.where(y == label), idx] = 1\n",
    "        return y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ (Input Matrix): This is a matrix where each row represents a data sample and each column represents a feature. If you have $n_{samples}$ data points and $n_{features}$ features, the dimension of $X$ would be $n_{samples} \\times n_{features}$.\n",
    "\n",
    "Weights: This is a vector with a length equal to the number of features ($n_{features}$). Its dimension is $n_{features} \\times 1.$\n",
    "\n",
    "Gradient descent is not calculating the coefficients of the sigmoid function but adjusting the weights $w_i$ and bias $b$ that are used within the linear combination fed into the sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  1  1  0  0  2  0  0  0  0]\n",
      " [ 0 17  1  0  0  1  0  0  0  0]\n",
      " [ 3  1 13  1  0  1  0  2  0  3]\n",
      " [ 1  0  0  9  5  0  1  1  2  4]\n",
      " [ 0  0  0  0  9  1  1  0  3  0]\n",
      " [ 1  4  0  1  0 20  0  2  1  0]\n",
      " [ 1  0  0  1  0  0 15  0  0  1]\n",
      " [ 0  0  0  0  0  0  0 18  2  1]\n",
      " [ 1  0  1  0  4  0  0  1 17  0]\n",
      " [ 4  0  0  1  1  1  3  0  0  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.48      0.71      0.57        14\n",
      "   classical       0.74      0.89      0.81        19\n",
      "     country       0.81      0.54      0.65        24\n",
      "       disco       0.69      0.39      0.50        23\n",
      "      hiphop       0.47      0.64      0.55        14\n",
      "        jazz       0.77      0.69      0.73        29\n",
      "       metal       0.75      0.83      0.79        18\n",
      "         pop       0.75      0.86      0.80        21\n",
      "      reggae       0.68      0.71      0.69        24\n",
      "        rock       0.31      0.29      0.30        14\n",
      "\n",
      "    accuracy                           0.66       200\n",
      "   macro avg       0.65      0.66      0.64       200\n",
      "weighted avg       0.67      0.66      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "model = MultinomialLogisticRegression(lr=0.01, n_iters=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert numeric predictions back to string labels\n",
    "label_mapping = {idx: label for idx, label in enumerate(model.classes_)}\n",
    "string_predictions = [label_mapping[label] for label in predictions]\n",
    "\n",
    "# Evaluate the model\n",
    "print(confusion_matrix(y_test, string_predictions))\n",
    "print(classification_report(y_test, string_predictions))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
